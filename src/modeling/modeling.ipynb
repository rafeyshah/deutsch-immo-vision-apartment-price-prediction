{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Model                  | Best for                     | Pros                              | Cons                         |\n",
    "|------------------------|------------------------------|-----------------------------------|------------------------------|\n",
    "| **Linear Regression**  | Simple, linear relationships | Fast, interpretable               | Limited to linear data, sensitive to outliers |\n",
    "| **Random Forest**      | Complex, tabular data        | Handles outliers, captures non-linear patterns | Slower, less interpretable |\n",
    "| **XGBoost**            | High-stakes, complex data    | High accuracy, handles non-linear data, regularization | Complex tuning, resource-intensive |\n",
    "\n",
    "\n",
    "In practice, you can start with Linear Regression if you expect a simple relationship. If itâ€™s not enough, try Random Forest. If you need the best possible performance and are okay with more tuning, use XGBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modeling Notebook - Dimensionality Reduction and Training\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the feature-engineered data\n",
    "file_path = '../../data/feature_engineered_immo_data.csv'  # Ensure this is correct\n",
    "data = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Define Features (X) and Target (y)\n",
    "X = data.drop(columns=['totalRent'])  # 'totalRent' is the target variable\n",
    "Y = data['totalRent']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of components selected to explain 95% variance: 1\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Apply PCA for Dimensionality Reduction (if needed)\n",
    "# Apply PCA to retain 95% of the variance, which will choose an optimal number of components\n",
    "pca = PCA(n_components=0.95)\n",
    "X_reduced = pca.fit_transform(X)\n",
    "\n",
    "# Print the number of components selected by PCA\n",
    "print(f\"Number of components selected to explain 95% variance: {X_reduced.shape[1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Split Data into Training and Testing Sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_reduced, Y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Initialize and Train Models\n",
    "# Define models to train\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Random Forest': RandomForestRegressor(random_state=42),\n",
    "    'XGBoost': XGBRegressor(random_state=42)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store results\n",
    "results = {}\n",
    "\n",
    "# Train, predict, and evaluate each model\n",
    "for model_name, model in models.items():\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    # Save results\n",
    "    results[model_name] = {\n",
    "        'MAE': mae,\n",
    "        'MSE': mse,\n",
    "        'R2 Score': r2\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Evaluation Results:\n",
      "Linear Regression: {'MAE': np.float64(193.0353279000988), 'MSE': np.float64(63904.140016557125), 'R2 Score': 0.004641938952950175}\n",
      "Random Forest: {'MAE': np.float64(222.36540682548014), 'MSE': np.float64(85210.09325856206), 'R2 Score': -0.32721531320984454}\n",
      "XGBoost: {'MAE': np.float64(192.59617854767689), 'MSE': np.float64(63250.71635227815), 'R2 Score': 0.014819534823115488}\n"
     ]
    }
   ],
   "source": [
    "# Display results\n",
    "print(\"Model Evaluation Results:\")\n",
    "for model, metrics in results.items():\n",
    "    print(f\"{model}: {metrics}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
